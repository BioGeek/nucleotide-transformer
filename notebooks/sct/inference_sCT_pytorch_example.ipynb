{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with sCellTransformer - Pytorch version from HuggingFace\n",
    "\n",
    "sCT was ported from `jax` to `PyTorch`, and then released through HuggingFace.  \n",
    "This notebook aims to ensure that the HuggingFace model works properly, by loading it and using it on dummy sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/nucleotide-transformer/blob/main/notebooks/sct/inference_sCT_pytorch_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:46:48.920336Z",
     "start_time": "2025-06-05T14:46:48.909784Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:58:39.191959Z",
     "start_time": "2025-06-05T14:58:39.179948Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import os\n",
    "import json\n",
    "import anndata as ad\n",
    "import boto3\n",
    "import botocore\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from scipy.sparse import issparse\n",
    "from typing import Any\n",
    "\n",
    "from nucleotide_transformer.sCellTransformer.model import sCT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:51:57.812358Z",
     "start_time": "2025-06-05T14:51:52.094920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the model from HuggingFace\n",
    "model = AutoModel.from_pretrained(\"InstaDeepAI/sCellTransformer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:56:12.024234Z",
     "start_time": "2025-06-05T14:55:52.672884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize S3 client with no signing configuration\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    config=botocore.config.Config(signature_version=botocore.UNSIGNED)\n",
    ")\n",
    "\n",
    "# Downloading the file from the public API of cellxgene\n",
    "# This file is a h5ad file containing single-cell RNA-seq data\n",
    "# It corresponds to Sst Chodl - MTG: Seattle Alzheimer's Disease Atlas (SEA-AD)\n",
    "# - Single-cell RNA-seq data: cells x genes expression matrix\n",
    "# - Sparse data (~90% zeros), typically 16k cells, ~30k genes\n",
    "# - Contains cell type annotations (neurons, astrocytes, etc.) and metadata\n",
    "# - Real biological data vs synthetic test data - will need preprocessing for model\n",
    "s3.download_file(\n",
    "    'cellxgene-data-public',\n",
    "    'cell-census/2023-12-15/h5ads/81e91ff8-f619-4ad1-a0c3-b45e1dc63f68.h5ad',\n",
    "    'brain.h5ad'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:17.415623Z",
     "start_time": "2025-06-05T14:57:17.382549Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loading mapping from ENSEMBL name to index in the dataset.\n",
    "with open(os.path.join(current_dir, \"data/ensembl_id_vocab.json\"), \"r\") as f:\n",
    "    ENSEMBL_ID_VOCAB = json.load(f)\n",
    "\n",
    "# Loading mapping, for the considered coding genes,\n",
    "# between global index in the dataset and their index among coding genes only.\n",
    "# Restricting from 60k genes to 20k genes only. \n",
    "with open(os.path.join(current_dir, \"data/protein_gene_map.json\"), \"r\") as f:\n",
    "    PROTEIN_GENE_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:19.363792Z",
     "start_time": "2025-06-05T14:57:18.606816Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_mapping_between_adata_and_model(adata: ad.AnnData,\n",
    "                                           ENSEMBL_ID_VOCAB: dict,\n",
    "                                           PROTEIN_GENE_MAP: dict) -> np.ndarray:\n",
    "    # Define mapping\n",
    "    names = list(adata.var.feature_name.keys())\n",
    "    MAP_TO_PROTEIN_GENE_INDEX = {}\n",
    "    indexes_present_in_data = {}\n",
    "    for i, name in enumerate(names):\n",
    "        if name in ENSEMBL_ID_VOCAB:\n",
    "            index = str(ENSEMBL_ID_VOCAB[name])\n",
    "            if index in PROTEIN_GENE_MAP:\n",
    "                indexes_present_in_data[index] = 1\n",
    "                MAP_TO_PROTEIN_GENE_INDEX[str(i)] = PROTEIN_GENE_MAP[index]\n",
    "\n",
    "    # Create gene mapping arrays\n",
    "    gene_map = {int(k): MAP_TO_PROTEIN_GENE_INDEX[k] for k in MAP_TO_PROTEIN_GENE_INDEX}\n",
    "    new_gene_map_array = np.full(70000, -1, dtype=np.int32)\n",
    "    for k, v in gene_map.items():\n",
    "        new_gene_map_array[k] = v\n",
    "    return new_gene_map_array\n",
    "\n",
    "\n",
    "# Note that this data download already includes a log normalization \n",
    "# on the gene expression levels.\n",
    "adata = sc.read_h5ad('brain.h5ad')\n",
    "# Creating the mapping between indexes in the downloaded dataset and \n",
    "# the index in the model for the considered coding genes. \n",
    "new_gene_map_array = define_mapping_between_adata_and_model(\n",
    "    adata=adata,\n",
    "    ENSEMBL_ID_VOCAB=ENSEMBL_ID_VOCAB,\n",
    "    PROTEIN_GENE_MAP=PROTEIN_GENE_MAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the data formatted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is formatted as a h5ad file.\n",
    "It contains:\n",
    "- data.X: It is a sparse matrix of gene expression levels (shape: num_cells x num_genes). \n",
    "- data.var[\"feature_name\"]: contains the name of the genes (shape: num_genes)\n",
    "- the data also contains many metadata columns such as the cell type, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:19.971925Z",
     "start_time": "2025-06-05T14:57:19.950583Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_h5ad_scrna_dataset(\n",
    "        adata: Any,\n",
    "        new_gene_map_array: np.ndarray,\n",
    "        num_downsamples: int,\n",
    "        cell_len: int,\n",
    "        num_cells: int,\n",
    "        pad_token_id: int,\n",
    "        gene_expression_num_bins: int,\n",
    "        batch_size: int,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Creates an iterable dataset from h5ad file for single-cell RNA-seq data.\n",
    "    \n",
    "    Args:\n",
    "        h5ad_path: Path to the h5ad file\n",
    "        new_gene_map_array: Array mapping new gene indices to your previous index system\n",
    "        num_downsamples: Number of downsampling steps\n",
    "        cell_len: Length of each cell in the dataset\n",
    "        num_cells: Number of cells per sample\n",
    "        pad_token_id: Token ID for padding\n",
    "        gene_expression_num_bins: Number of bins for gene expression\n",
    "        batch_size: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        An iterable dataset that yields batches of samples\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract expression matrix (usually X is sparse)\n",
    "    expr_matrix = adata.X\n",
    "    if issparse(expr_matrix):\n",
    "        # Convert to CSR for efficient row access\n",
    "        expr_matrix = expr_matrix.tocsr()\n",
    "\n",
    "    # Calculate sequence length with downsampling\n",
    "    downsample_factor = 2 ** num_downsamples\n",
    "    seq_length = math.ceil(cell_len / downsample_factor) * downsample_factor\n",
    "\n",
    "    class H5adIterableDataset:\n",
    "        def __init__(self):\n",
    "            self.length = cell_len\n",
    "            self.batch_size = batch_size\n",
    "            self.total_cells = expr_matrix.shape[0]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.length\n",
    "\n",
    "        def __iter__(self):\n",
    "            # Create infinite iterator over cell indices\n",
    "            cell_indices = itertools.cycle(range(self.total_cells))\n",
    "\n",
    "            while True:\n",
    "                batch = []\n",
    "\n",
    "                # Generate batch_size samples\n",
    "                for _ in range(self.batch_size):\n",
    "                    cells = []\n",
    "\n",
    "                    # Collect num_cells cells for one sample\n",
    "                    while len(cells) < num_cells:\n",
    "                        cell_idx = next(cell_indices)\n",
    "\n",
    "                        # Get expression data for this cell\n",
    "                        if issparse(expr_matrix):\n",
    "                            # For sparse matrix, get the row as a dense array\n",
    "                            cell_expr = expr_matrix[cell_idx, :].toarray().flatten()\n",
    "                        else:\n",
    "                            cell_expr = expr_matrix[cell_idx, :]\n",
    "\n",
    "                        # Find non-zero expressions\n",
    "                        non_zero_mask = cell_expr > 0\n",
    "                        gene_idxs = np.where(non_zero_mask)[0].astype(np.int32)\n",
    "                        expressions = cell_expr[non_zero_mask].astype(np.float32)\n",
    "\n",
    "                        if len(gene_idxs) == 0:\n",
    "                            # Skip cells with no expression\n",
    "                            continue\n",
    "\n",
    "                        # Map genes using new_gene_map_array\n",
    "                        mapped_idxs = new_gene_map_array[gene_idxs]\n",
    "                        valid_mask = mapped_idxs != -1\n",
    "                        positions = mapped_idxs[valid_mask]\n",
    "                        valid_expr = expressions[valid_mask]\n",
    "\n",
    "                        if len(valid_expr) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Bin expressions\n",
    "                        if min(valid_expr) == max(valid_expr):\n",
    "                            bin_edges = np.array(\n",
    "                                [min(valid_expr) - 0.1, max(valid_expr) + 0.1])\n",
    "                            binned = np.ones_like(valid_expr, dtype=np.int32)\n",
    "                        else:\n",
    "                            bin_edges = np.linspace(\n",
    "                                min(valid_expr),\n",
    "                                max(valid_expr),\n",
    "                                gene_expression_num_bins,\n",
    "                            )\n",
    "                            bin_edges[-1] += 0.01\n",
    "                            binned = np.digitize(valid_expr, bin_edges)\n",
    "\n",
    "                        # Create full arrays (using the original gene_ids from your code)\n",
    "                        full_expr = np.zeros(self.length, dtype=np.int32)\n",
    "                        full_expr[positions] = binned\n",
    "\n",
    "                        raw_expr = np.zeros(self.length, dtype=np.float32)\n",
    "                        raw_expr[positions] = valid_expr\n",
    "\n",
    "                        # Create cell dictionary\n",
    "                        cell = {\n",
    "                            # \"gene_ids\": gene_ids.copy(),\n",
    "                            \"gene_expressions\": full_expr,\n",
    "                            \"raw_gene_expressions\": raw_expr,\n",
    "                            \"bins\": bin_edges,\n",
    "                            \"source\": [\"h5ad\"],\n",
    "                        }\n",
    "\n",
    "                        # Pad if needed\n",
    "                        if seq_length > self.length:\n",
    "                            for key in [\"gene_expressions\",\n",
    "                                        \"raw_gene_expressions\"]:\n",
    "                                pad_value = pad_token_id\n",
    "                                padded = np.full(seq_length, pad_value,\n",
    "                                                 dtype=cell[key].dtype)\n",
    "                                padded[:len(cell[key])] = cell[key]\n",
    "                                cell[key] = padded\n",
    "\n",
    "                        cells.append(cell)\n",
    "\n",
    "                    # Merge cells for this sample\n",
    "                    if len(cells) == num_cells:\n",
    "                        sample = {}\n",
    "\n",
    "                        # Merge arrays by concatenation\n",
    "                        for key in cells[0]:\n",
    "                            if isinstance(cells[0][key], np.ndarray):\n",
    "                                sample[key] = np.concatenate([c[key] for c in cells])\n",
    "                            else:\n",
    "                                sample[key] = list(itertools.chain.from_iterable(\n",
    "                                    c[key] for c in cells\n",
    "                                ))\n",
    "\n",
    "                        batch.append(sample)\n",
    "\n",
    "                # Reshape and yield batch\n",
    "                if len(batch) == self.batch_size:\n",
    "                    batch_result = {}\n",
    "\n",
    "                    for key in batch[0]:\n",
    "                        if isinstance(batch[0][key], np.ndarray):\n",
    "                            batch_arrays = [b[key] for b in batch]\n",
    "                            batch_result[key] = np.stack(batch_arrays, axis=0)\n",
    "                        else:\n",
    "                            batch_result[key] = [b[key] for b in batch]\n",
    "\n",
    "                    yield batch_result\n",
    "\n",
    "    return H5adIterableDataset()\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataloader = get_h5ad_scrna_dataset(\n",
    "    adata=adata,\n",
    "    new_gene_map_array=new_gene_map_array,\n",
    "    num_downsamples=model.config.num_downsamples,\n",
    "    cell_len=len(PROTEIN_GENE_MAP),\n",
    "    num_cells=model.config.num_cells,\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    gene_expression_num_bins=5,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:06:26.816392Z",
     "start_time": "2025-06-05T15:06:26.810515Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_gene_expression_imputation(\n",
    "        model: sCT,\n",
    "        train_dataloader: DataLoader,\n",
    "        num_batches: int|None = 10,\n",
    "        mask_ratio: float = 0.15,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a gene expression imputation model using Matthews Correlation Coefficient.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model for gene expression imputation\n",
    "        train_dataloader: DataLoader containing gene expression data\n",
    "        num_batches: Number of batches to evaluate. If None, all batches are evaluated\n",
    "        mask_ratio: Ratio of tokens to mask for imputation\n",
    "\n",
    "    Returns:\n",
    "        float: Average Matthews Correlation Coefficient across all batches\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Lists to store true and predicted values for masked tokens\n",
    "    all_true_values = []\n",
    "    all_pred_values = []\n",
    "\n",
    "    # Iterate over specified number of batches\n",
    "    iterator = iter(train_dataloader)\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches)):\n",
    "        try:\n",
    "            # Get next batch\n",
    "            batch = next(iterator)\n",
    "\n",
    "            # Move batch to the same device as model\n",
    "            device = next(model.parameters()).device\n",
    "            gene_expressions = torch.tensor(batch[\"gene_expressions\"], device=device)\n",
    "\n",
    "            # Create random mask\n",
    "            mask = torch.rand(gene_expressions.shape, device=device) < mask_ratio\n",
    "\n",
    "            # Clone and mask gene expressions\n",
    "            masked_gene_expressions = gene_expressions.clone()\n",
    "            masked_gene_expressions[mask] = model.config.mask_token_id\n",
    "\n",
    "            # Keep original values before masking for evaluation\n",
    "            true_values = gene_expressions[mask].detach().cpu().numpy()\n",
    "\n",
    "            # Forward pass without gradient computation\n",
    "            with torch.no_grad():\n",
    "                outputs = model(masked_gene_expressions)\n",
    "\n",
    "            # Extract logits from model output (adapt based on your model's output format)\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = outputs.get(\"logits\", outputs)\n",
    "            else:\n",
    "                logits = outputs\n",
    "\n",
    "            # Get predictions (assuming classification - adjust if regression)\n",
    "            predictions = torch.argmax(logits[:,:,:,:5], dim=-1)\n",
    "            pred_values = predictions[mask].detach().cpu().numpy()\n",
    "\n",
    "            # Store true and predicted values\n",
    "            all_true_values.append(true_values)\n",
    "            all_pred_values.append(pred_values)\n",
    "\n",
    "        except StopIteration:\n",
    "            print(f\"DataLoader exhausted after {batch_idx} batches\")\n",
    "            break\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_true_values = np.concatenate(all_true_values)\n",
    "    all_pred_values = np.concatenate(all_pred_values)\n",
    "\n",
    "    # Compute Matthews Correlation Coefficient\n",
    "    # - Binary classification metric ranging from -1 to +1\n",
    "    # - +1: perfect prediction, 0: random prediction, -1: total disagreement\n",
    "    # - Balanced metric that works well with imbalanced datasets\n",
    "    # - Considers all confusion matrix elements (TP, TN, FP, FN)\n",
    "    mcc = matthews_corrcoef(all_true_values, all_pred_values)\n",
    "    print(f\"Overall MCC: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:08:35.672183Z",
     "start_time": "2025-06-05T15:06:40.768788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Define the number of batches and the mask ratio for evaluation\n",
    "# The reported metric is the average MCC across all batches computed over the bins.\n",
    "evaluate_gene_expression_imputation(\n",
    "    model,\n",
    "    dataloader,\n",
    "    num_batches=None,\n",
    "    mask_ratio=0.15,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomics_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
